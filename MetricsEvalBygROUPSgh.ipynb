{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERE GROUPING PROCESSINHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 matching filenames.\n",
      "Groupings extracted and saved to: /Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import re\n",
    "showcase_dir = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"\n",
    "groupings_output = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"\n",
    "\n",
    "# 1) Define multiple regex patterns\n",
    "patterns = [\n",
    "    # a) original pattern: something containing \"_cv.jpg\" and ending \"_gt.png\"\n",
    "    re.compile(r'_cv\\.jpg.*_gt\\.png$', re.IGNORECASE),\n",
    "    \n",
    "    # b) pattern for e.g. \"34_45_r0_d0_cv.jpggt-Chicago\"\n",
    "    #    or \"61_52_r0_d1_cv.jpggt-Mexico.png\"\n",
    "    #    We look for \"_cv.jpggt-\" + (some city text) + optional extension\n",
    "    #    NOTE: Adjust character class [^.]+ if city names can have spaces, etc.\n",
    "    re.compile(r'_cv\\.jpggt-[^.]+(\\.png|\\.jpg|\\.jpeg|\\.tiff|\\.bmp)?$', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "groupings_list = []\n",
    "\n",
    "for fname in os.listdir(showcase_dir):\n",
    "    # Skip directories\n",
    "    if os.path.isdir(os.path.join(showcase_dir, fname)):\n",
    "        continue\n",
    "    \n",
    "    # Check each pattern; if any matches, we collect it\n",
    "    for pat in patterns:\n",
    "        if pat.search(fname):\n",
    "            groupings_list.append(fname)\n",
    "            break  # stop checking other patterns once matched\n",
    "\n",
    "# Save to JSON\n",
    "with open(groupings_output, 'w') as f:\n",
    "    json.dump(groupings_list, f, indent=2)\n",
    "\n",
    "print(f\"Found {len(groupings_list)} matching filenames.\")\n",
    "print(f\"Groupings extracted and saved to: {groupings_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUPING + GENERATED IMAGE PATHS MAPPINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexico\n",
      "Chicago\n",
      "Found 299 matching filenames.\n",
      "Groupings extracted and saved to: /Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define your directories\n",
    "showcase_dir = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"\n",
    "groupings_output = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"\n",
    "\n",
    "# 1) Define multiple regex patterns\n",
    "patterns = [\n",
    "    # a) original pattern: something containing \"_cv.jpg\" and ending with \"_gt.png\"\n",
    "    re.compile(r'_cv\\.jpg.*_gt\\.png$', re.IGNORECASE),\n",
    "    \n",
    "    # b) pattern for e.g. \"34_45_r0_d0_cv.jpggt-Chicago\" or \"61_52_r0_d1_cv.jpggt-gt-Mexico.png\"\n",
    "    #    This regex captures the city name (removing the extension).\n",
    "    re.compile(r'_cv\\.jpggt-(.+?)(\\.png|\\.jpg|\\.jpeg|\\.tiff|\\.bmp)$', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "# This list will hold dictionaries for each grouping.\n",
    "groupings_list = []\n",
    "\n",
    "for fname in os.listdir(showcase_dir):\n",
    "    # Skip directories\n",
    "    if os.path.isdir(os.path.join(showcase_dir, fname)):\n",
    "        continue\n",
    "    \n",
    "    # Check each pattern; if any matches, we process this file.\n",
    "    for pat in patterns:\n",
    "        m = pat.search(fname)\n",
    "        if m:\n",
    "            # Initialize variables\n",
    "            base = None\n",
    "            city = None\n",
    "\n",
    "            # Case 1: filenames like \"14_16_r0_d0_cv.jpgStockholm._gt.png\"\n",
    "            if \"_cv.jpggt-\" in fname:\n",
    "                base = fname.split(\"_cv.jpggt-\")[0]\n",
    "                city = m.group(1)  # capture group from regex\n",
    "                print(city)\n",
    "                # If the city string starts with \"gt-\", remove it.\n",
    "                if city.lower().startswith(\"gt-\"):\n",
    "                    city = city[3:]\n",
    "            elif \"_cv.jpg\" in fname:\n",
    "                base = fname.split(\"_cv.jpg\")[0]\n",
    "                remainder = fname.split(\"_cv.jpg\")[1]\n",
    "                if remainder.endswith(\"_gt.png\"):\n",
    "                    city = remainder[:-len(\"_gt.png\")]\n",
    "                else:\n",
    "                    city = remainder\n",
    "            # Case 2: filenames like \"34_45_r0_d0_cv.jpggt-Chicago\" or \"61_52_r0_d1_cv.jpggt-gt-Mexico.png\"\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Clean up the city string (remove extra whitespace and trailing punctuation such as . or _ or -)\n",
    "            city = city.strip().rstrip(\"._-\")\n",
    "\n",
    "            # Create generated image paths using the specified pattern:\n",
    "            # {base}___{city}____{i}.png   for i in 0, 1, 2.\n",
    "            generated_paths = [f\"{base}___{city}____{i}.png\" for i in range(3)]\n",
    "            grouping_item = {\n",
    "                \"original\": fname,\n",
    "                \"base\": base,\n",
    "                \"city\": city,\n",
    "                \"generated\": generated_paths\n",
    "            }\n",
    "            groupings_list.append(grouping_item)\n",
    "            break\n",
    "\n",
    "output_data = {\"groupings\": groupings_list}\n",
    "\n",
    "with open(groupings_output, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Found {len(groupings_list)} matching filenames.\")\n",
    "print(f\"Groupings extracted and saved to: {groupings_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By group, ground truth vs generated, per group per specific city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangzhuoyulucas/anaconda3/envs/cnet_gud/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 299 groupings.\n",
      "Using 5 worker(s) for CPU processing (threads).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing groupings:   0%|          | 0/299 [00:00<?, ?it/s]/Users/wangzhuoyulucas/anaconda3/envs/cnet_gud/lib/python3.12/site-packages/torchmetrics/functional/image/lpips.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n",
      "Processing groupings: 100%|██████████| 299/299 [04:59<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to /Users/wangzhuoyulucas/SMART /generatedImg/groupings_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# Global base directory (all images are stored here)\n",
    "base_dir = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Function: Load and Preprocess Image\n",
    "# -------------------------------\n",
    "def load_image(image_path, device):\n",
    "    \"\"\"\n",
    "    Load an image from disk, resize to 224x224, convert to tensor, and send it to device.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions to Compute Metrics from Preloaded Tensors\n",
    "# -------------------------------\n",
    "def compute_l2_distance_tensor(img1, img2):\n",
    "    return torchmetrics.functional.pairwise_euclidean_distance(img1.flatten(1), img2.flatten(1)).item()\n",
    "\n",
    "def compute_psnr_tensor(img1, img2, device):\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio().to(device)\n",
    "    return psnr_metric(img1, img2).item()\n",
    "\n",
    "def compute_ssim_tensor(img1, img2, device):\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure().to(device)\n",
    "    return ssim_metric(img1, img2).item()\n",
    "\n",
    "def compute_lpips_tensor(img1, img2, device):\n",
    "    lpips_metric = torchmetrics.image.LearnedPerceptualImagePatchSimilarity(net_type='alex').to(device)\n",
    "    return lpips_metric(img1, img2).item()\n",
    "\n",
    "# -------------------------------\n",
    "# Function to Process a Single Grouping\n",
    "# -------------------------------\n",
    "def process_grouping(grouping):\n",
    "    \"\"\"\n",
    "    Process one grouping: load the original and generated images (from base_dir),\n",
    "    compute metrics for each generated image against the original, and return a dictionary\n",
    "    with the results.\n",
    "    \n",
    "    The returned dict has keys:\n",
    "      - base, city\n",
    "      - For each metric in [\"L2\", \"PSNR\", \"SSIM\", \"LPIPS\"]:\n",
    "          * img{i}_{metric} for each generated image (i starting at 1)\n",
    "          * mean_{metric}, max_{metric}, up25per_{metric}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # We'll use CPU for all metric calculations.\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "        # Retrieve info from the grouping.\n",
    "        original_filename = grouping[\"original\"]\n",
    "        base_name = grouping.get(\"base\", \"\")\n",
    "        city = grouping.get(\"city\", \"\")\n",
    "    \n",
    "        original_path = os.path.join(base_dir, original_filename)\n",
    "        try:\n",
    "            orig_tensor = load_image(original_path, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading original image {original_filename}: {e}\")\n",
    "            return None\n",
    "    \n",
    "        # List to hold metric dictionaries for each generated image.\n",
    "        gen_metrics = []\n",
    "        for gen_filename in grouping.get(\"generated\", []):\n",
    "            generated_path = os.path.join(base_dir, gen_filename)\n",
    "            try:\n",
    "                gen_tensor = load_image(generated_path, device)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading generated image {gen_filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "            # Compute metrics between original and generated image.\n",
    "            l2   = compute_l2_distance_tensor(orig_tensor, gen_tensor)\n",
    "            psnr = compute_psnr_tensor(orig_tensor, gen_tensor, device)\n",
    "            ssim = compute_ssim_tensor(orig_tensor, gen_tensor, device)\n",
    "            lpips = compute_lpips_tensor(orig_tensor, gen_tensor, device)\n",
    "    \n",
    "            gen_metrics.append({\n",
    "                \"L2\": l2,\n",
    "                \"PSNR\": psnr,\n",
    "                \"SSIM\": ssim,\n",
    "                \"LPIPS\": lpips\n",
    "            })\n",
    "    \n",
    "        # Prepare the row dictionary to be inserted into the DataFrame.\n",
    "        row = {\"base\": base_name, \"city\": city}\n",
    "        metrics_names = [\"L2\", \"PSNR\", \"SSIM\", \"LPIPS\"]\n",
    "    \n",
    "        # For each metric, store individual generated image values and compute aggregate stats.\n",
    "        for metric in metrics_names:\n",
    "            # Extract the list of values for this metric.\n",
    "            values = [m[metric] for m in gen_metrics]\n",
    "            # Save individual image metric values as columns (img1_{metric}, img2_{metric}, etc.)\n",
    "            for i, val in enumerate(values):\n",
    "                row[f\"img{i+1}_{metric}\"] = val\n",
    "            # If there are fewer than 3 generated images, fill missing columns with None.\n",
    "            for i in range(len(values), 3):\n",
    "                row[f\"img{i+1}_{metric}\"] = None\n",
    "    \n",
    "            # Compute aggregated statistics if values are available.\n",
    "            if values:\n",
    "                row[f\"mean_{metric}\"]   = np.mean(values)\n",
    "                row[f\"max_{metric}\"]    = np.max(values)\n",
    "                row[f\"up25per_{metric}\"] = np.percentile(values, 75)  # 75th percentile = upper 25%\n",
    "            else:\n",
    "                row[f\"mean_{metric}\"]   = None\n",
    "                row[f\"max_{metric}\"]    = None\n",
    "                row[f\"up25per_{metric}\"] = None\n",
    "    \n",
    "        return row\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_grouping: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------\n",
    "# Main Processing: Parallel over Groupings and Save Results\n",
    "# -------------------------------\n",
    "def main():\n",
    "    # Path to your grouping JSON file.\n",
    "    groupings_file = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"\n",
    "    \n",
    "    # Load grouping data.\n",
    "    with open(groupings_file, 'r') as f:\n",
    "        groupings_data = json.load(f)\n",
    "    groupings = groupings_data.get(\"groupings\", [])\n",
    "    print(f\"Loaded {len(groupings)} groupings.\")\n",
    "    \n",
    "    # Determine number of worker threads: available CPUs minus 3 (at least 1 worker).\n",
    "    num_workers = max(os.cpu_count() - 3, 1)\n",
    "    print(f\"Using {num_workers} worker(s) for CPU processing (threads).\")\n",
    "    \n",
    "    results = []\n",
    "    # Use ThreadPoolExecutor to avoid pickling issues in interactive environments.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Create futures and wrap with tqdm for a progress bar.\n",
    "        futures = [executor.submit(process_grouping, grouping) for grouping in groupings]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing groupings\"):\n",
    "            try:\n",
    "                row = future.result()\n",
    "                if row is not None:\n",
    "                    results.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Worker raised an exception: {e}\")\n",
    "    \n",
    "    # Create a DataFrame from the results.\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Optionally, reorder columns so that base and city come first.\n",
    "    cols = df.columns.tolist()\n",
    "    ordered_cols = [col for col in [\"base\", \"city\"] if col in cols] + [col for col in cols if col not in [\"base\", \"city\"]]\n",
    "    df = df[ordered_cols]\n",
    "    \n",
    "    # Save the DataFrame to a CSV file.\n",
    "    output_csv = \"/Users/wangzhuoyulucas/SMART /generatedImg/groupings_metrics.csv\"\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved results to {output_csv}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the csv to get by city metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated image metrics saved to /Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics_by_city_image.csv\n",
      "   Category  Metrics    over-all     Chicago    HongKong     Kigali  \\\n",
      "0        L2        N  299.000000   28.000000   26.000000  28.000000   \n",
      "1        L2     mean  107.665870  128.592145  110.178674  80.310286   \n",
      "2        L2      max  113.930284  137.288838  114.751334  82.454910   \n",
      "3        L2  up25per  110.351644  131.794583  112.887297  81.473838   \n",
      "4      PSNR        N  299.000000   28.000000   26.000000  28.000000   \n",
      "5      PSNR     mean   11.353975    9.632400   11.202151  13.785047   \n",
      "6      PSNR      max   11.831617   10.114967   11.683435  14.061837   \n",
      "7      PSNR  up25per   11.620405    9.963504   11.408916  13.910833   \n",
      "8      SSIM        N  299.000000   28.000000   26.000000  28.000000   \n",
      "9      SSIM     mean    0.114218    0.083126    0.094877   0.149616   \n",
      "10     SSIM      max    0.128970    0.097078    0.105875   0.159843   \n",
      "11     SSIM  up25per    0.121684    0.090622    0.101054   0.154761   \n",
      "12    LPIPS        N  299.000000   28.000000   26.000000  28.000000   \n",
      "13    LPIPS     mean    0.462700    0.466563    0.519168   0.434939   \n",
      "14    LPIPS      max    0.486777    0.494309    0.546132   0.447753   \n",
      "15    LPIPS  up25per    0.474192    0.479788    0.532862   0.440396   \n",
      "\n",
      "     Kinshasa      Mexico      Munich     Orlando    SaoPaulo   Singapore  \\\n",
      "0   35.000000   31.000000   22.000000   43.000000   30.000000   29.000000   \n",
      "1   74.150775  117.341791  102.281745   97.654594  144.075190  130.939125   \n",
      "2   78.238015  121.970129  106.904886  111.301138  148.161389  138.295623   \n",
      "3   76.623749  119.494809  104.374779  102.312847  146.072809  134.306401   \n",
      "4   35.000000   31.000000   22.000000   43.000000   30.000000   29.000000   \n",
      "5   14.487922   10.399688   11.582037   11.877630    8.620495    9.502627   \n",
      "6   15.055598   10.709558   11.976226   12.755649    8.860099    9.958568   \n",
      "7   14.699936   10.574136   11.795641   12.479003    8.741025    9.748111   \n",
      "8   35.000000   31.000000   22.000000   43.000000   30.000000   29.000000   \n",
      "9    0.166002    0.056844    0.133090    0.179283    0.032416    0.092487   \n",
      "10   0.190974    0.066573    0.150262    0.199705    0.036465    0.110733   \n",
      "11   0.175031    0.061579    0.142398    0.192607    0.034509    0.100469   \n",
      "12  35.000000   31.000000   22.000000   43.000000   30.000000   29.000000   \n",
      "13   0.426071    0.437881    0.481298    0.445699    0.470559    0.521169   \n",
      "14   0.451200    0.453172    0.502878    0.490771    0.488197    0.542444   \n",
      "15   0.440584    0.445501    0.492415    0.463439    0.478891    0.532085   \n",
      "\n",
      "     Stockholm  \n",
      "0    27.000000  \n",
      "1    99.128612  \n",
      "2   104.300729  \n",
      "3   101.096974  \n",
      "4    27.000000  \n",
      "5    11.803429  \n",
      "6    12.274826  \n",
      "7    12.076471  \n",
      "8    27.000000  \n",
      "9     0.122352  \n",
      "10    0.135905  \n",
      "11    0.129135  \n",
      "12   27.000000  \n",
      "13    0.449478  \n",
      "14    0.466129  \n",
      "15    0.458317  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the CSV created from your groupings\n",
    "input_csv = \"/Users/wangzhuoyulucas/SMART /generatedImg/groupings_metrics.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# List of image metrics you computed in your groupings\n",
    "metrics = [\"L2\", \"PSNR\", \"SSIM\", \"LPIPS\"]\n",
    "\n",
    "# Aggregated statistic types we want (similar to your evaluation CSV for population)\n",
    "# Here we include:\n",
    "# - \"N\": the number of groupings (count)\n",
    "# - \"mean\": the average of the per-grouping aggregated value (e.g., mean_L2)\n",
    "# - \"max\": the average of the per-grouping max value (you might also consider taking the overall max)\n",
    "# - \"up25per\": the average of the per-grouping 75th percentile (upper 25%)\n",
    "agg_stats = [\"N\", \"mean\", \"max\", \"up25per\"]\n",
    "\n",
    "# First, compute overall (all groupings) aggregated stats.\n",
    "overall = {}\n",
    "overall[\"N\"] = len(df)\n",
    "for metric in metrics:\n",
    "    overall[f\"mean_{metric}\"]   = df[f\"mean_{metric}\"].mean()\n",
    "    overall[f\"max_{metric}\"]    = df[f\"max_{metric}\"].mean()      # or use .max() if desired\n",
    "    overall[f\"up25per_{metric}\"] = df[f\"up25per_{metric}\"].mean()\n",
    "\n",
    "# Get a sorted list of cities present in the CSV\n",
    "cities = sorted(df[\"city\"].unique())\n",
    "\n",
    "# Compute aggregated stats for each city.\n",
    "city_results = {}\n",
    "for city in cities:\n",
    "    city_df = df[df[\"city\"] == city]\n",
    "    stats = {}\n",
    "    stats[\"N\"] = len(city_df)\n",
    "    for metric in metrics:\n",
    "        stats[f\"mean_{metric}\"]   = city_df[f\"mean_{metric}\"].mean()\n",
    "        stats[f\"max_{metric}\"]    = city_df[f\"max_{metric}\"].mean()\n",
    "        stats[f\"up25per_{metric}\"] = city_df[f\"up25per_{metric}\"].mean()\n",
    "    city_results[city] = stats\n",
    "\n",
    "# Now build a new DataFrame with rows for each metric and aggregator type.\n",
    "rows = []\n",
    "for metric in metrics:\n",
    "    for agg in agg_stats:\n",
    "        row = {\"Category\": metric, \"Metrics\": agg}\n",
    "        # Overall column:\n",
    "        if agg == \"N\":\n",
    "            row[\"over-all\"] = overall[\"N\"]\n",
    "        else:\n",
    "            row[\"over-all\"] = overall[f\"{agg}_{metric}\"]\n",
    "        # Columns for each city:\n",
    "        for city in cities:\n",
    "            if agg == \"N\":\n",
    "                row[city] = city_results[city][\"N\"]\n",
    "            else:\n",
    "                row[city] = city_results[city][f\"{agg}_{metric}\"]\n",
    "        rows.append(row)\n",
    "\n",
    "# Create the aggregated DataFrame.\n",
    "agg_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: reorder the columns if you want \"Category\" and \"Metrics\" first,\n",
    "# then \"over-all\" and then each city.\n",
    "cols = [\"Category\", \"Metrics\", \"over-all\"] + cities\n",
    "agg_df = agg_df[cols]\n",
    "\n",
    "# Save to a new CSV.\n",
    "output_csv = \"/Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics_by_city_image.csv\"\n",
    "agg_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Aggregated image metrics saved to {output_csv}\")\n",
    "print(agg_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIVERSITY CHECKS, generated2generated, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 299 groupings.\n",
      "Using 5 worker(s) (threads) for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing groupings:   0%|          | 0/299 [00:00<?, ?it/s]/Users/wangzhuoyulucas/anaconda3/envs/cnet_gud/lib/python3.12/site-packages/torchmetrics/functional/image/lpips.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n",
      "Processing groupings: 100%|██████████| 299/299 [02:29<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to /Users/wangzhuoyulucas/SMART /generatedImg/groupings_generated_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "# Import ms_ssim from pytorch_msssim (install with pip install pytorch-msssim)\n",
    "from pytorch_msssim import ms_ssim\n",
    "\n",
    "# Global base directory (all images are stored here)\n",
    "base_dir = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Function: Load and Preprocess Image\n",
    "# -------------------------------\n",
    "def load_image(image_path, device):\n",
    "    \"\"\"\n",
    "    Load an image from disk, resize to 224x224, convert to tensor (range [0,1]),\n",
    "    and add a batch dimension.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Function to Process a Single Grouping for Generated Images\n",
    "# -------------------------------\n",
    "def process_generated_grouping(grouping):\n",
    "    \"\"\"\n",
    "    For a grouping containing three generated images, compute pairwise comparisons\n",
    "    for SSIM, MS-SSIM, and LPIPS. The comparisons are:\n",
    "      - img1 vs. img2  (prefix \"img1_2\")\n",
    "      - img2 vs. img3  (prefix \"img2_3\")\n",
    "      - img1 vs. img3  (prefix \"img1_3\")\n",
    "    Then compute the mean over the three pairs.\n",
    "    Returns a dict with:\n",
    "      - base, city\n",
    "      - For each metric:\n",
    "          * e.g. \"img1_2SSIM\", \"img2_3SSIM\", \"img1_3SSIM\" and \"mean_SSIM\"\n",
    "          * similarly for \"MS_SSIM\" and \"LPIPS\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = torch.device(\"cpu\")\n",
    "        base_name = grouping.get(\"base\", \"\")\n",
    "        city = grouping.get(\"city\", \"\")\n",
    "        gen_filenames = grouping.get(\"generated\", [])\n",
    "        \n",
    "        # Ensure exactly three generated images: if less, fill with None; if more, take first three.\n",
    "        if len(gen_filenames) < 3:\n",
    "            gen_filenames = gen_filenames + [None]*(3 - len(gen_filenames))\n",
    "        elif len(gen_filenames) > 3:\n",
    "            gen_filenames = gen_filenames[:3]\n",
    "        \n",
    "        # Load the three images.\n",
    "        images = []\n",
    "        for fname in gen_filenames:\n",
    "            if fname is None:\n",
    "                images.append(None)\n",
    "            else:\n",
    "                full_path = os.path.join(base_dir, fname)\n",
    "                try:\n",
    "                    img_tensor = load_image(full_path, device)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading generated image {fname}: {e}\")\n",
    "                    img_tensor = None\n",
    "                images.append(img_tensor)\n",
    "        \n",
    "        # If any image is missing, skip metric computation.\n",
    "        if any(img is None for img in images):\n",
    "            row = {\"base\": base_name, \"city\": city}\n",
    "            for metric in [\"SSIM\", \"MS_SSIM\", \"LPIPS\"]:\n",
    "                row[\"img1_2\" + metric] = None\n",
    "                row[\"img2_3\" + metric] = None\n",
    "                row[\"img1_3\" + metric] = None\n",
    "                row[\"mean_\" + metric] = None\n",
    "            return row\n",
    "\n",
    "        # Initialize torchmetrics for SSIM and LPIPS.\n",
    "        ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "        lpips_metric = torchmetrics.image.LearnedPerceptualImagePatchSimilarity(net_type='alex')\n",
    "        \n",
    "        # Define the three pairs and their prefixes.\n",
    "        pairs = [\n",
    "            ((0, 1), \"img1_2\"),\n",
    "            ((1, 2), \"img2_3\"),\n",
    "            ((0, 2), \"img1_3\")\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        for (i, j), prefix in pairs:\n",
    "            # Compute SSIM\n",
    "            ssim_val = ssim_metric(images[i], images[j]).item()\n",
    "            # Compute MS-SSIM using pytorch-msssim (images already in [0,1], shape (1, C, H, W))\n",
    "            ms_ssim_val = ms_ssim(images[i], images[j], data_range=1.0, size_average=True).item()\n",
    "            # Compute LPIPS\n",
    "            lpips_val = lpips_metric(images[i], images[j]).item()\n",
    "            results[prefix + \"SSIM\"] = ssim_val\n",
    "            results[prefix + \"MS_SSIM\"] = ms_ssim_val\n",
    "            results[prefix + \"LPIPS\"] = lpips_val\n",
    "        \n",
    "        # Compute mean for each metric.\n",
    "        mean_ssim = np.mean([results[\"img1_2SSIM\"], results[\"img2_3SSIM\"], results[\"img1_3SSIM\"]])\n",
    "        mean_ms_ssim = np.mean([results[\"img1_2MS_SSIM\"], results[\"img2_3MS_SSIM\"], results[\"img1_3MS_SSIM\"]])\n",
    "        mean_lpips = np.mean([results[\"img1_2LPIPS\"], results[\"img2_3LPIPS\"], results[\"img1_3LPIPS\"]])\n",
    "        \n",
    "        row = {\"base\": base_name, \"city\": city}\n",
    "        row.update(results)\n",
    "        row[\"mean_SSIM\"] = mean_ssim\n",
    "        row[\"mean_MS_SSIM\"] = mean_ms_ssim\n",
    "        row[\"mean_LPIPS\"] = mean_lpips\n",
    "        \n",
    "        return row\n",
    "    except Exception as e:\n",
    "        print(\"Error in process_generated_grouping:\", e)\n",
    "        return None\n",
    "\n",
    "# -------------------------------\n",
    "# Main Processing Pipeline\n",
    "# -------------------------------\n",
    "def main():\n",
    "    # Path to your grouping JSON file (update path as needed)\n",
    "    groupings_file = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"\n",
    "    with open(groupings_file, 'r') as f:\n",
    "        groupings_data = json.load(f)\n",
    "    groupings = groupings_data.get(\"groupings\", [])\n",
    "    print(f\"Loaded {len(groupings)} groupings.\")\n",
    "    \n",
    "    # Use a ThreadPoolExecutor to process in parallel.\n",
    "    num_workers = max(os.cpu_count() - 3, 1)\n",
    "    print(f\"Using {num_workers} worker(s) (threads) for processing.\")\n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_generated_grouping, grouping) for grouping in groupings]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures),\n",
    "                           total=len(futures),\n",
    "                           desc=\"Processing groupings\"):\n",
    "            try:\n",
    "                row = future.result()\n",
    "                if row is not None:\n",
    "                    results.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Worker raised an exception: {e}\")\n",
    "    \n",
    "    # Create DataFrame from results.\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Optionally reorder columns so that 'base' and 'city' are first.\n",
    "    cols = df.columns.tolist()\n",
    "    ordered_cols = [col for col in [\"base\", \"city\"] if col in cols] + [col for col in cols if col not in [\"base\", \"city\"]]\n",
    "    df = df[ordered_cols]\n",
    "    \n",
    "    # Save the DataFrame to CSV in a writable directory.\n",
    "    output_csv = os.path.join(\"/Users/wangzhuoyulucas/SMART /generatedImg/groupings_generated_metrics.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved results to {output_csv}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency ResNet Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Settings and Device\n",
    "# -------------------------------\n",
    "# For prediction on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transformation (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# For de-normalization in prediction (max values)\n",
    "max_values = [135.9, 5379.6, 38447.0]\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# Define model architecture (ResNet50 with regression head)\n",
    "model = models.resnet50(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 3),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# Load trained weights (update path as needed)\n",
    "model.load_state_dict(torch.load(\"/Users/wangzhuoyulucas/SMART /generatedImg/resnet50_regression.pth\",\n",
    "                                 map_location=device,\n",
    "                                 weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset for Generated Images (using groupings_newFID.json)\n",
    "# -------------------------------\n",
    "class GeneratedImagesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset loads generated images from a folder based on the list of generated\n",
    "    filenames in a groupings JSON file (groupings_newFID.json). It filters out any filenames\n",
    "    that contain \"tifcon.png\".\n",
    "    \n",
    "    Expected generated filename format (from groupings):\n",
    "      \"61_52_r0_d1__Mexico___0.png\"\n",
    "    where:\n",
    "      - The base is the substring before the first \"__\"\n",
    "      - The city is the substring between \"__\" and \"___\"\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder, groupings_json_path, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the groupings JSON file and collect generated filenames\n",
    "        with open(groupings_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        groupings = data.get(\"groupings\", [])\n",
    "        \n",
    "        filenames = []\n",
    "        for grouping in groupings:\n",
    "            filenames.extend(grouping.get(\"generated\", []))\n",
    "        # Remove duplicates\n",
    "        filenames = list(set(filenames))\n",
    "        # Filter out filenames that contain \"tifcon.png\"\n",
    "        self.filenames = [f for f in filenames if \"tifcon.png\" not in f.lower()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        img_path = os.path.join(self.image_folder, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, fname\n",
    "\n",
    "def extract_base_and_city(fname):\n",
    "    \"\"\"\n",
    "    Expected filename format: base__city___index.png\n",
    "    Returns (base, city)\n",
    "    \"\"\"\n",
    "    parts = fname.split(\"__\")\n",
    "    if len(parts) >= 2:\n",
    "        base = parts[0]\n",
    "        city_part = parts[1]\n",
    "        city = city_part.split(\"___\")[0]\n",
    "    else:\n",
    "        base = fname\n",
    "        city = \"\"\n",
    "    return base, city\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # -------------------------------\n",
    "    # Prediction Pipeline for Generated Images\n",
    "    # -------------------------------\n",
    "    generated_images_folder = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"  # update as needed\n",
    "    groupings_json_path = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"  # update as needed\n",
    "\n",
    "    # For interactive environments, set num_workers=0; for standalone scripts, you can use a higher number.\n",
    "    num_workers = 0\n",
    "\n",
    "    dataset = GeneratedImagesDataset(generated_images_folder, groupings_json_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    predictions = []\n",
    "    print(\"Predicting on generated images...\")\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(dataloader, desc=\"Predicting\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            # Denormalize predictions: multiply each output by corresponding max_value and convert to float\n",
    "            predicted_values = [float(outputs[i] * max_values[i]) for i in range(3)]\n",
    "            fname = filenames[0]\n",
    "            base, city = extract_base_and_city(fname)\n",
    "            predictions.append({\n",
    "                \"generated_image\": os.path.join(generated_images_folder, fname),\n",
    "                \"base\": base,\n",
    "                \"city\": city,\n",
    "                \"predicted_values\": predicted_values\n",
    "            })\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ground Truth Matching\n",
    "    # -------------------------------\n",
    "    # Create mapping from base to official city using groupings_newFID.json\n",
    "    with open(groupings_json_path, 'r') as f:\n",
    "        groupings_data = json.load(f)\n",
    "    groupings_list = groupings_data.get(\"groupings\", [])\n",
    "    base_to_city = {}\n",
    "    for grouping in groupings_list:\n",
    "        b = grouping.get(\"base\", \"\").lower()\n",
    "        c = grouping.get(\"city\", \"\").lower()\n",
    "        if b:\n",
    "            base_to_city[b] = c\n",
    "\n",
    "    # Load the \"all.json\" file containing ground truth info (assumes JSON Lines format)\n",
    "    all_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/src/all.json\"  # update as needed\n",
    "    all_data = []\n",
    "    with open(all_json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                all_data.append(json.loads(line))\n",
    "\n",
    "    # Function to extract three numbers from prompt (assumes order: area, volume, population)\n",
    "    def extract_numbers_from_prompt(prompt):\n",
    "        numbers = re.findall(r\"[\\d]+\\.[\\d]+\", prompt)\n",
    "        if len(numbers) >= 3:\n",
    "            return [float(numbers[0]), float(numbers[1]), float(numbers[2])]\n",
    "        return None\n",
    "\n",
    "    final_results = []\n",
    "    for pred in predictions:\n",
    "        base = pred[\"base\"]\n",
    "        # Use the mapping to get the official city from the groupings\n",
    "        city = base_to_city.get(base.lower(), pred[\"city\"].lower())\n",
    "        match = None\n",
    "        for item in all_data:\n",
    "            target_path = item.get(\"target\", \"\").lower()\n",
    "            # Both base and official city must appear in the target path.\n",
    "            if (base.lower() in target_path) and (city in target_path):\n",
    "                match = item\n",
    "                break\n",
    "        if match is not None:\n",
    "            ground_truth = extract_numbers_from_prompt(match.get(\"prompt\", \"\"))\n",
    "            target_path = match.get(\"target\", \"\")\n",
    "            if ground_truth is not None:\n",
    "                ground_truth = [float(val) for val in ground_truth]\n",
    "        else:\n",
    "            ground_truth = None\n",
    "            target_path = None\n",
    "\n",
    "        final_results.append({\n",
    "            \"base\": base,\n",
    "            \"city\": city,\n",
    "            \"generated_image\": pred[\"generated_image\"],\n",
    "            \"predicted_values\": pred[\"predicted_values\"],\n",
    "            \"target_path\": target_path,\n",
    "            \"ground_truth\": ground_truth\n",
    "        })\n",
    "\n",
    "    # -------------------------------\n",
    "    # Save Final Results\n",
    "    # -------------------------------\n",
    "    output_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\"\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "\n",
    "    print(f\"Final results saved to {output_json_path}\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Predicting on generated images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 897/897 [01:05<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results saved to /Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Settings and Device\n",
    "# -------------------------------\n",
    "# For prediction on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transformation (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# For de-normalization in prediction (max values)\n",
    "max_values = [135.9, 5379.6, 38447.0]\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# Define model architecture (ResNet50 with regression head)\n",
    "model = models.resnet50(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 3),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# Load trained weights (update path as needed)\n",
    "model.load_state_dict(torch.load(\"/Users/wangzhuoyulucas/SMART /generatedImg/resnet50_regression.pth\",\n",
    "                                 map_location=device,\n",
    "                                 weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset for Generated Images (using groupings_newFID.json)\n",
    "# -------------------------------\n",
    "class GeneratedImagesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset loads generated images from a folder based on the list of generated\n",
    "    filenames in a groupings JSON file (groupings_newFID.json). It filters out any filenames\n",
    "    that contain \"tifcon.png\".\n",
    "    \n",
    "    Expected generated filename format (from groupings):\n",
    "      \"61_52_r0_d1__Mexico___0.png\"\n",
    "    where:\n",
    "      - The base is the substring before the first \"__\"\n",
    "      - The city is the substring between \"__\" and \"___\"\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder, groupings_json_path, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the groupings JSON file and collect generated filenames\n",
    "        with open(groupings_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        groupings = data.get(\"groupings\", [])\n",
    "        \n",
    "        filenames = []\n",
    "        for grouping in groupings:\n",
    "            filenames.extend(grouping.get(\"generated\", []))\n",
    "        # Remove duplicates\n",
    "        filenames = list(set(filenames))\n",
    "        # Filter out filenames that contain \"tifcon.png\"\n",
    "        self.filenames = [f for f in filenames if \"tifcon.png\" not in f.lower()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        img_path = os.path.join(self.image_folder, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, fname\n",
    "\n",
    "def extract_base_and_city_style(fname):\n",
    "    \"\"\"\n",
    "    Expected filename format: base__city_style___index.png\n",
    "    Returns (base, city_style)\n",
    "    \"\"\"\n",
    "    parts = fname.split(\"__\")\n",
    "    if len(parts) >= 2:\n",
    "        base = parts[0]\n",
    "        city_style = parts[1].split(\"___\")[0]\n",
    "    else:\n",
    "        base = fname\n",
    "        city_style = \"\"\n",
    "    return base, city_style\n",
    "\n",
    "# -------------------------------\n",
    "# Create mapping from base to official city using groupings_newFID.json\n",
    "# -------------------------------\n",
    "with open(\"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\", 'r') as f:\n",
    "    groupings_data = json.load(f)\n",
    "groupings_list = groupings_data.get(\"groupings\", [])\n",
    "# Create a dictionary mapping base (in lowercase) to the official city (in lowercase)\n",
    "base_to_city = {}\n",
    "for grouping in groupings_list:\n",
    "    b = grouping.get(\"base\", \"\").lower()\n",
    "    c = grouping.get(\"city\", \"\").lower()\n",
    "    if b:\n",
    "        base_to_city[b] = c\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # -------------------------------\n",
    "    # Prediction Pipeline for Generated Images\n",
    "    # -------------------------------\n",
    "    # Set the folder that contains generated images (update path as needed)\n",
    "    generated_images_folder = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"  # update as needed\n",
    "    groupings_json_path = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"   # update as needed\n",
    "\n",
    "    # For interactive environments, set num_workers=0; for scripts, you can use a higher number (e.g. 5).\n",
    "    num_workers = 0\n",
    "\n",
    "    dataset = GeneratedImagesDataset(generated_images_folder, groupings_json_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = []  # list to store prediction results\n",
    "    print(\"Predicting on generated images...\")\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(dataloader, desc=\"Predicting\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            # Denormalize predictions: multiply each output by corresponding max_value and convert to float\n",
    "            predicted_values = [float(outputs[i] * max_values[i]) for i in range(3)]\n",
    "            \n",
    "            # Extract base and city_style from the filename.\n",
    "            fname = filenames[0]\n",
    "            base, city_style = extract_base_and_city_style(fname)\n",
    "            \n",
    "            predictions.append({\n",
    "                \"generated_image\": os.path.join(generated_images_folder, fname),\n",
    "                \"base\": base,\n",
    "                \"city_style\": city_style,\n",
    "                \"predicted_values\": predicted_values\n",
    "            })\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ground Truth Matching\n",
    "    # -------------------------------\n",
    "    # Load the \"all.json\" file containing ground truth info (JSON Lines format)\n",
    "    all_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/src/all.json\"  # update path as needed\n",
    "    all_data = []\n",
    "    with open(all_json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                all_data.append(json.loads(line))\n",
    "\n",
    "    # Function to extract three numbers from prompt (assumes order: area, volume, population)\n",
    "    def extract_numbers_from_prompt(prompt):\n",
    "        numbers = re.findall(r\"[\\d]+\\.[\\d]+\", prompt)\n",
    "        if len(numbers) >= 3:\n",
    "            return [float(numbers[0]), float(numbers[1]), float(numbers[2])]\n",
    "        return None\n",
    "\n",
    "    # For each prediction, update the city using the groupings mapping and search for a matching ground truth in all_data.\n",
    "    final_results = []\n",
    "    for pred in predictions:\n",
    "        base = pred[\"base\"]\n",
    "        # Use the mapping to get the official city (if available)\n",
    "        city = base_to_city.get(base.lower(), \"\").lower()\n",
    "        # Also keep the city_style as extracted from the filename\n",
    "        city_style = pred[\"city_style\"].lower()\n",
    "        match = None\n",
    "        for item in all_data:\n",
    "            target_path = item.get(\"target\", \"\").lower()\n",
    "            # Check if both base and the official city are in the target path.\n",
    "            if (base.lower() in target_path) and (city in target_path):\n",
    "                match = item\n",
    "                break\n",
    "        if match is not None:\n",
    "            ground_truth = extract_numbers_from_prompt(match.get(\"prompt\", \"\"))\n",
    "            target_path = match.get(\"target\", \"\")\n",
    "            if ground_truth is not None:\n",
    "                ground_truth = [float(val) for val in ground_truth]\n",
    "        else:\n",
    "            ground_truth = None\n",
    "            target_path = None\n",
    "\n",
    "        final_results.append({\n",
    "            \"base\": base,\n",
    "            \"city_style\": city_style,   # extracted from filename\n",
    "            \"city\": city,               # official city from groupings\n",
    "            \"generated_image\": pred[\"generated_image\"],\n",
    "            \"predicted_values\": pred[\"predicted_values\"],\n",
    "            \"target_path\": target_path,\n",
    "            \"ground_truth\": ground_truth\n",
    "        })\n",
    "\n",
    "    # -------------------------------\n",
    "    # Save Final Results\n",
    "    # -------------------------------\n",
    "    output_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\"\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "\n",
    "    print(f\"Final results saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with prompt also extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Predicting on generated images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 897/897 [01:13<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results saved to /Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Settings and Device\n",
    "# -------------------------------\n",
    "# For prediction on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transformation (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# For de-normalization in prediction (max values)\n",
    "max_values = [135.9, 5379.6, 38447.0]\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# Define model architecture (ResNet50 with regression head)\n",
    "model = models.resnet50(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 3),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# Load trained weights (update path as needed)\n",
    "model.load_state_dict(torch.load(\"/Users/wangzhuoyulucas/SMART /generatedImg/resnet50_regression.pth\",\n",
    "                                 map_location=device,\n",
    "                                 weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset for Generated Images (using groupings_newFID.json)\n",
    "# -------------------------------\n",
    "class GeneratedImagesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset loads generated images from a folder based on the list of generated\n",
    "    filenames in a groupings JSON file (groupings_newFID.json). It filters out any filenames\n",
    "    that contain \"tifcon.png\".\n",
    "    \n",
    "    Expected generated filename format (from groupings):\n",
    "      \"61_52_r0_d1__Mexico___0.png\"\n",
    "    where:\n",
    "      - The base is the substring before the first \"__\"\n",
    "      - The city is the substring between \"__\" and \"___\"\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder, groupings_json_path, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the groupings JSON file and collect generated filenames\n",
    "        with open(groupings_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        groupings = data.get(\"groupings\", [])\n",
    "        \n",
    "        filenames = []\n",
    "        for grouping in groupings:\n",
    "            filenames.extend(grouping.get(\"generated\", []))\n",
    "        # Remove duplicates\n",
    "        filenames = list(set(filenames))\n",
    "        # Filter out filenames that contain \"tifcon.png\"\n",
    "        self.filenames = [f for f in filenames if \"tifcon.png\" not in f.lower()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        img_path = os.path.join(self.image_folder, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, fname\n",
    "\n",
    "def extract_base_and_city_style(fname):\n",
    "    \"\"\"\n",
    "    Expected filename format: base__city_style___index.png\n",
    "    Returns (base, city_style)\n",
    "    \"\"\"\n",
    "    parts = fname.split(\"__\")\n",
    "    if len(parts) >= 2:\n",
    "        base = parts[0]\n",
    "        city_style = parts[1].split(\"___\")[0]\n",
    "    else:\n",
    "        base = fname\n",
    "        city_style = \"\"\n",
    "    return base, city_style\n",
    "\n",
    "# -------------------------------\n",
    "# Create mapping from base to official city using groupings_newFID.json\n",
    "# -------------------------------\n",
    "with open(\"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\", 'r') as f:\n",
    "    groupings_data = json.load(f)\n",
    "groupings_list = groupings_data.get(\"groupings\", [])\n",
    "# Create a dictionary mapping base (in lowercase) to the official city (in lowercase)\n",
    "base_to_city = {}\n",
    "for grouping in groupings_list:\n",
    "    b = grouping.get(\"base\", \"\").lower()\n",
    "    c = grouping.get(\"city\", \"\").lower()\n",
    "    if b:\n",
    "        base_to_city[b] = c\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # -------------------------------\n",
    "    # Prediction Pipeline for Generated Images\n",
    "    # -------------------------------\n",
    "    # Set the folder that contains generated images (update path as needed)\n",
    "    generated_images_folder = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"  # update as needed\n",
    "    groupings_json_path = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"   # update as needed\n",
    "\n",
    "    # For interactive environments, set num_workers=0; for scripts, you can use a higher number (e.g. 5).\n",
    "    num_workers = 0\n",
    "\n",
    "    dataset = GeneratedImagesDataset(generated_images_folder, groupings_json_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = []  # list to store prediction results\n",
    "    print(\"Predicting on generated images...\")\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(dataloader, desc=\"Predicting\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            # Denormalize predictions: multiply each output by corresponding max_value and convert to float\n",
    "            predicted_values = [float(outputs[i] * max_values[i]) for i in range(3)]\n",
    "            \n",
    "            # Extract base and city_style from the filename.\n",
    "            fname = filenames[0]\n",
    "            base, city_style = extract_base_and_city_style(fname)\n",
    "            \n",
    "            predictions.append({\n",
    "                \"generated_image\": os.path.join(generated_images_folder, fname),\n",
    "                \"base\": base,\n",
    "                \"city_style\": city_style,\n",
    "                \"predicted_values\": predicted_values\n",
    "            })\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ground Truth Matching\n",
    "    # -------------------------------\n",
    "    # Load the \"all.json\" file containing ground truth info (JSON Lines format)\n",
    "    all_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/src/all.json\"  # update path as needed\n",
    "    all_data = []\n",
    "    with open(all_json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                all_data.append(json.loads(line))\n",
    "\n",
    "    # Function to extract three numbers from prompt (assumes order: area, volume, population)\n",
    "    def extract_numbers_from_prompt(prompt):\n",
    "        numbers = re.findall(r\"[\\d]+\\.[\\d]+\", prompt)\n",
    "        if len(numbers) >= 3:\n",
    "            return [float(numbers[0]), float(numbers[1]), float(numbers[2])]\n",
    "        return None\n",
    "\n",
    "    # For each prediction, update the city using the groupings mapping and search for a matching ground truth in all_data.\n",
    "    final_results = []\n",
    "    for pred in predictions:\n",
    "        base = pred[\"base\"]\n",
    "        # Use the mapping to get the official city (if available)\n",
    "        city = base_to_city.get(base.lower(), \"\").lower()\n",
    "        # Also keep the city_style as extracted from the filename\n",
    "        city_style = pred[\"city_style\"].lower()\n",
    "        match = None\n",
    "        for item in all_data:\n",
    "            target_path = item.get(\"target\", \"\").lower()\n",
    "            # Check if both base and the official city are in the target path.\n",
    "            if (base.lower() in target_path) and (city in target_path):\n",
    "                match = item\n",
    "                break\n",
    "        if match is not None:\n",
    "            prompt = match.get(\"prompt\", \"\")\n",
    "            ground_truth = extract_numbers_from_prompt(prompt)\n",
    "            target_path = match.get(\"target\", \"\")\n",
    "            if ground_truth is not None:\n",
    "                ground_truth = [float(val) for val in ground_truth]\n",
    "        else:\n",
    "            ground_truth = None\n",
    "            target_path = None\n",
    "            prompt = None\n",
    "\n",
    "        final_results.append({\n",
    "            \"base\": base,\n",
    "            \"city_style\": city_style,   # extracted from filename\n",
    "            \"city\": city,               # official city from groupings\n",
    "            \"generated_image\": pred[\"generated_image\"],\n",
    "            \"predicted_values\": pred[\"predicted_values\"],\n",
    "            \"target_path\": target_path,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"prompt\": prompt          # added prompt from the matched item\n",
    "        })\n",
    "\n",
    "    # -------------------------------\n",
    "    # Save Final Results\n",
    "    # -------------------------------\n",
    "    output_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\"\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "\n",
    "    print(f\"Final results saved to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With clip score calculated (897 images about 40 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Predicting on generated images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 897/897 [00:55<00:00, 16.25it/s]\n",
      "Computing CLIP Scores: 100%|██████████| 897/897 [29:45<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results with CLIP scores saved to /Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_with_clip.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "\n",
    "# -------------------------------\n",
    "# Settings and Device\n",
    "# -------------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Transformation for Prediction (ResNet)\n",
    "# -------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "max_values = [135.9, 5379.6, 38447.0]\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup: ResNet50 with Regression Head\n",
    "# -------------------------------\n",
    "model = models.resnet50(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 3),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model.load_state_dict(torch.load(\"/Users/wangzhuoyulucas/SMART /generatedImg/resnet50_regression.pth\",\n",
    "                                 map_location=device,\n",
    "                                 weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset for Generated Images\n",
    "# -------------------------------\n",
    "class GeneratedImagesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads generated images based on filenames from a groupings JSON file.\n",
    "    Filters out any filenames that contain \"tifcon.png\".\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder, groupings_json_path, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(groupings_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        groupings = data.get(\"groupings\", [])\n",
    "        \n",
    "        filenames = []\n",
    "        for grouping in groupings:\n",
    "            filenames.extend(grouping.get(\"generated\", []))\n",
    "        filenames = list(set(filenames))\n",
    "        self.filenames = [f for f in filenames if \"tifcon.png\" not in f.lower()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        img_path = os.path.join(self.image_folder, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, fname\n",
    "\n",
    "def extract_base_and_city_style(fname):\n",
    "    \"\"\"\n",
    "    Expected filename format: base__city_style___index.png\n",
    "    Returns (base, city_style)\n",
    "    \"\"\"\n",
    "    parts = fname.split(\"__\")\n",
    "    if len(parts) >= 2:\n",
    "        base = parts[0]\n",
    "        city_style = parts[1].split(\"___\")[0]\n",
    "    else:\n",
    "        base = fname\n",
    "        city_style = \"\"\n",
    "    return base, city_style\n",
    "\n",
    "# -------------------------------\n",
    "# Mapping from Base to Official City\n",
    "# -------------------------------\n",
    "with open(\"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\", 'r') as f:\n",
    "    groupings_data = json.load(f)\n",
    "groupings_list = groupings_data.get(\"groupings\", [])\n",
    "base_to_city = {}\n",
    "for grouping in groupings_list:\n",
    "    b = grouping.get(\"base\", \"\").lower()\n",
    "    c = grouping.get(\"city\", \"\").lower()\n",
    "    if b:\n",
    "        base_to_city[b] = c\n",
    "\n",
    "# -------------------------------\n",
    "# Main Pipeline\n",
    "# -------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    generated_images_folder = \"/Users/wangzhuoyulucas/SMART /generatedImg/show-case-FID\"\n",
    "    groupings_json_path = \"/Users/wangzhuoyulucas/SMART /data_server/urban_data/tencities/GenAI_density/groupings_newFID.json\"\n",
    "    all_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/src/all.json\"\n",
    "    \n",
    "    # Create dataset and dataloader for prediction\n",
    "    num_workers = 0  # For interactive environments; adjust as needed\n",
    "    dataset = GeneratedImagesDataset(generated_images_folder, groupings_json_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Prediction: Run the ResNet model on generated images\n",
    "    # -------------------------------\n",
    "    predictions = []\n",
    "    print(\"Predicting on generated images...\")\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(dataloader, desc=\"Predicting\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            # Denormalize predictions\n",
    "            predicted_values = [float(outputs[i] * max_values[i]) for i in range(3)]\n",
    "            fname = filenames[0]\n",
    "            base, city_style = extract_base_and_city_style(fname)\n",
    "            predictions.append({\n",
    "                \"generated_image\": os.path.join(generated_images_folder, fname),\n",
    "                \"base\": base,\n",
    "                \"city_style\": city_style,\n",
    "                \"predicted_values\": predicted_values\n",
    "            })\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Ground Truth Matching and Prompt Extraction\n",
    "    # -------------------------------\n",
    "    all_data = []\n",
    "    with open(all_json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                all_data.append(json.loads(line))\n",
    "    \n",
    "    def extract_numbers_from_prompt(prompt):\n",
    "        numbers = re.findall(r\"[\\d]+\\.[\\d]+\", prompt)\n",
    "        if len(numbers) >= 3:\n",
    "            return [float(numbers[0]), float(numbers[1]), float(numbers[2])]\n",
    "        return None\n",
    "    \n",
    "    final_results = []\n",
    "    for pred in predictions:\n",
    "        base = pred[\"base\"]\n",
    "        city = base_to_city.get(base.lower(), \"\").lower()\n",
    "        city_style = pred[\"city_style\"].lower()\n",
    "        match = None\n",
    "        for item in all_data:\n",
    "            target_path = item.get(\"target\", \"\").lower()\n",
    "            if (base.lower() in target_path) and (city in target_path):\n",
    "                match = item\n",
    "                break\n",
    "        if match is not None:\n",
    "            prompt = match.get(\"prompt\", \"\")\n",
    "            ground_truth = extract_numbers_from_prompt(prompt)\n",
    "            target_path = match.get(\"target\", \"\")\n",
    "            if ground_truth is not None:\n",
    "                ground_truth = [float(val) for val in ground_truth]\n",
    "        else:\n",
    "            ground_truth = None\n",
    "            target_path = None\n",
    "            prompt = None\n",
    "    \n",
    "        final_results.append({\n",
    "            \"base\": base,\n",
    "            \"city_style\": city_style,\n",
    "            \"city\": city,\n",
    "            \"generated_image\": pred[\"generated_image\"],\n",
    "            \"predicted_values\": pred[\"predicted_values\"],\n",
    "            \"target_path\": target_path,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"prompt\": prompt\n",
    "        })\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Define a simple transform for CLIP (resize to 224x224 and convert to tensor)\n",
    "    # Note: transforms.ToTensor() will produce values between 0 and 1.\n",
    "    # Thus, we set do_rescale=False when creating the CLIPScore metric.\n",
    "    clip_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Compute CLIP Scores for Each Result using torchmetrics (CPU, Parallelized)\n",
    "    # -------------------------------\n",
    "    # We instantiate a new CLIPScore metric for each sample to avoid the warning\n",
    "    # about calling compute before update.\n",
    "    def compute_clip_score_for_result(result):\n",
    "        if result[\"prompt\"] is None:\n",
    "            result[\"clip_score\"] = None\n",
    "            return result\n",
    "        try:\n",
    "            image = Image.open(result[\"generated_image\"]).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {result['generated_image']}: {e}\")\n",
    "            result[\"clip_score\"] = None\n",
    "            return result\n",
    "        image_tensor = clip_transform(image).unsqueeze(0).to(device)  # shape: (1, 3, 224, 224)\n",
    "        # Instantiate a new CLIPScore metric with do_rescale=False for each sample.\n",
    "        clip_metric_instance = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\").to(device)\n",
    "        score = clip_metric_instance(image_tensor, result[\"prompt\"])\n",
    "        result[\"clip_score\"] = score.item()\n",
    "        return result\n",
    "    \n",
    "    # Use ThreadPoolExecutor with tqdm progress bar to process CLIP score computation in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        final_results = list(tqdm(\n",
    "            executor.map(compute_clip_score_for_result, final_results),\n",
    "            total=len(final_results),\n",
    "            desc=\"Computing CLIP Scores\"\n",
    "        ))\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Save Final Results with CLIP Scores\n",
    "    # -------------------------------\n",
    "    output_json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_with_clip.json\"\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Final results with CLIP scores saved to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the clips to get top 5 bottom 5 and by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall clip_score statistics:\n",
      "  Mean: 17.51\n",
      "  Max: 19.12\n",
      "  25th Percentile: 17.28\n",
      "  75th Percentile: 17.77\n",
      "\n",
      "Mean clip_score per city:\n",
      "        city  mean_clip_score\n",
      "0    chicago        17.324560\n",
      "1   hongkong        17.437904\n",
      "2     kigali        16.383949\n",
      "3   kinshasa        17.364836\n",
      "4     mexico        17.547715\n",
      "5     munich        17.668256\n",
      "6    orlando        18.446414\n",
      "7   saopaulo        17.373903\n",
      "8  singapore        17.562303\n",
      "9  stockholm        17.533081\n",
      "\n",
      "Detailed top/bottom rows saved to: /Users/wangzhuoyulucas/SMART /generatedImg/clip_score_top_bottom_by_city.csv\n",
      "Summary statistics saved to: /Users/wangzhuoyulucas/SMART /generatedImg/clip_score_summary.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/n6tgn0rs16bfp1cm36gy56hh0000gn/T/ipykernel_27859/2430995901.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ranked = df.groupby('city', group_keys=False).apply(assign_top_bottom)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load the JSON file into a DataFrame\n",
    "# -------------------------------\n",
    "json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_with_clip.json\"\n",
    "df = pd.read_json(json_path)\n",
    "\n",
    "# Ensure that 'clip_score' is numeric (it might be None or invalid in some rows)\n",
    "df['clip_score'] = pd.to_numeric(df['clip_score'], errors='coerce')\n",
    "\n",
    "# Optionally, filter out rows where clip_score is NaN:\n",
    "df = df.dropna(subset=['clip_score'])\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute overall statistics for clip_score\n",
    "# -------------------------------\n",
    "overall_mean = df['clip_score'].mean()\n",
    "overall_max = df['clip_score'].max()\n",
    "q25 = df['clip_score'].quantile(0.25)\n",
    "q75 = df['clip_score'].quantile(0.75)\n",
    "\n",
    "print(\"Overall clip_score statistics:\")\n",
    "print(f\"  Mean: {overall_mean:.2f}\")\n",
    "print(f\"  Max: {overall_max:.2f}\")\n",
    "print(f\"  25th Percentile: {q25:.2f}\")\n",
    "print(f\"  75th Percentile: {q75:.2f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute mean clip_score per city\n",
    "# -------------------------------\n",
    "city_means = df.groupby('city', as_index=False)['clip_score'].mean().rename(columns={'clip_score': 'mean_clip_score'})\n",
    "print(\"\\nMean clip_score per city:\")\n",
    "print(city_means)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. For each city, get top 5 and bottom 5 entries by clip_score\n",
    "# -------------------------------\n",
    "def assign_top_bottom(group):\n",
    "    \"\"\"\n",
    "    For each city's group, mark the top 5 rows (highest clip_score) as 'top5'\n",
    "    and the bottom 5 rows (lowest clip_score) as 'bottom5'. If the group has fewer than 10 rows,\n",
    "    rows may be in both categories.\n",
    "    \"\"\"\n",
    "    group = group.copy()\n",
    "    group = group.sort_values('clip_score', ascending=False)\n",
    "    group['ranking'] = ''\n",
    "    top5_index = group.head(5).index\n",
    "    bottom5_index = group.tail(5).index\n",
    "    group.loc[top5_index, 'ranking'] = 'top5'\n",
    "    group.loc[bottom5_index, 'ranking'] = 'bottom5'\n",
    "    return group\n",
    "\n",
    "# Apply the ranking function per city.\n",
    "df_ranked = df.groupby('city', group_keys=False).apply(assign_top_bottom)\n",
    "\n",
    "# If you want to store only the top5 and bottom5 rows per city (i.e. remove any rows that are not in these groups)\n",
    "df_top_bottom = df_ranked[df_ranked['ranking'] != '']\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save the results to CSV files\n",
    "# -------------------------------\n",
    "# Save the top5 and bottom5 detailed rows (with all columns) to CSV.\n",
    "output_csv_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/clip_score_top_bottom_by_city.csv\"\n",
    "df_top_bottom.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nDetailed top/bottom rows saved to: {output_csv_path}\")\n",
    "\n",
    "# Optionally, save overall and per-city summary stats to another CSV.\n",
    "summary_csv_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/clip_score_summary.csv\"\n",
    "summary_stats = pd.DataFrame({\n",
    "    \"overall_mean\": [overall_mean],\n",
    "    \"overall_max\": [overall_max],\n",
    "    \"25th_percentile\": [q25],\n",
    "    \"75th_percentile\": [q75]\n",
    "})\n",
    "# Save city-level means and overall stats (they can be stored in separate sheets or files; here we append them)\n",
    "with pd.ExcelWriter(summary_csv_path.replace('.csv', '.xlsx')) as writer:\n",
    "    city_means.to_excel(writer, sheet_name=\"City_Means\", index=False)\n",
    "    summary_stats.to_excel(writer, sheet_name=\"Overall_Stats\", index=False)\n",
    "print(f\"Summary statistics saved to: {summary_csv_path.replace('.csv', '.xlsx')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r2 for CONSISTENCY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/wangzhuoyulucas/anaconda3/envs/cnet_gud/lib/python3.12/site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/wangzhuoyulucas/anaconda3/envs/cnet_gud/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics saved to /Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics.csv\n",
      "                        Target        R2           MSE          MAE  \\\n",
      "0  Total built-up surface area  0.888232  6.546947e+01     5.830976   \n",
      "1        Total built-up volume  0.500068  9.611077e+04   199.384547   \n",
      "2                   Population  0.458813  5.048156e+06  1128.489559   \n",
      "\n",
      "          RMSE  \n",
      "0     8.091321  \n",
      "1   310.017366  \n",
      "2  2246.810225  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the final predictions JSON\n",
    "with open(\"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Initialize lists for each target:\n",
    "gt_surface = []  # total built-up surface area\n",
    "pred_surface = []\n",
    "\n",
    "gt_volume = []   # total built-up volume\n",
    "pred_volume = []\n",
    "\n",
    "gt_population = []  # population in this area\n",
    "pred_population = []\n",
    "\n",
    "# Loop through each prediction and extract values if ground_truth exists.\n",
    "for item in results:\n",
    "    gt = item.get(\"ground_truth\")\n",
    "    pred = item.get(\"predicted_values\")\n",
    "    # Only include items where both ground truth and predicted values are available.\n",
    "    if gt is not None and pred is not None and len(gt) >= 3 and len(pred) >= 3:\n",
    "        gt_surface.append(gt[0])\n",
    "        pred_surface.append(pred[0])\n",
    "        \n",
    "        gt_volume.append(gt[1])\n",
    "        pred_volume.append(pred[1])\n",
    "        \n",
    "        gt_population.append(gt[2])\n",
    "        pred_population.append(pred[2])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "gt_surface = np.array(gt_surface)\n",
    "pred_surface = np.array(pred_surface)\n",
    "\n",
    "gt_volume = np.array(gt_volume)\n",
    "pred_volume = np.array(pred_volume)\n",
    "\n",
    "gt_population = np.array(gt_population)\n",
    "pred_population = np.array(pred_population)\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return r2, mse, mae, rmse\n",
    "\n",
    "# Compute metrics for each target\n",
    "metrics_surface = compute_metrics(gt_surface, pred_surface)\n",
    "metrics_volume = compute_metrics(gt_volume, pred_volume)\n",
    "metrics_population = compute_metrics(gt_population, pred_population)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "data = {\n",
    "    \"Target\": [\"Total built-up surface area\", \"Total built-up volume\", \"Population\"],\n",
    "    \"R2\": [metrics_surface[0], metrics_volume[0], metrics_population[0]],\n",
    "    \"MSE\": [metrics_surface[1], metrics_volume[1], metrics_population[1]],\n",
    "    \"MAE\": [metrics_surface[2], metrics_volume[2], metrics_population[2]],\n",
    "    \"RMSE\": [metrics_surface[3], metrics_volume[3], metrics_population[3]]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Evaluation metrics saved to {output_csv_path}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By city and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics saved to /Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics_by_city.csv\n",
      "       Target       City    N        R2         RMSE          MAE\n",
      "0  Population        All  897  0.458813  2246.810225  1128.489559\n",
      "1  Population    chicago   87 -0.074850   661.527800   436.411361\n",
      "2  Population   hongkong   78  0.377684  5399.925436  3628.771625\n",
      "3  Population     kigali   84  0.571203   624.471638   368.343939\n",
      "4  Population   kinshasa  102  0.586586  2638.008609  1830.260227\n",
      "5  Population     mexico   90  0.414827   857.510764   676.250258\n",
      "6  Population     munich   63  0.391949   631.853814   412.703545\n",
      "7  Population    orlando  129  0.068959   220.264716   147.973980\n",
      "8  Population   saopaulo   93 -0.116943  1192.303402   920.969016\n",
      "9  Population  singapore   87 -2.123055  3656.419918  2627.721783\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the final predictions JSON\n",
    "json_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/final_predictions_ResNet.json\"\n",
    "with open(json_path, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Initialize lists for overall metrics\n",
    "overall_surface_gt = []\n",
    "overall_surface_pred = []\n",
    "overall_volume_gt = []\n",
    "overall_volume_pred = []\n",
    "overall_population_gt = []\n",
    "overall_population_pred = []\n",
    "\n",
    "# Dictionary to hold per-city data.\n",
    "# Each city key will map to a dictionary holding lists for each target and a count N.\n",
    "cities_data = {}\n",
    "\n",
    "for item in results:\n",
    "    city = item.get(\"city\")\n",
    "    gt = item.get(\"ground_truth\")\n",
    "    pred = item.get(\"predicted_values\")\n",
    "    \n",
    "    # Only include items with valid ground_truth and predictions (with at least 3 values)\n",
    "    if gt is None or pred is None or len(gt) < 3 or len(pred) < 3:\n",
    "        continue\n",
    "    \n",
    "    # Append values to overall lists\n",
    "    overall_surface_gt.append(gt[0])\n",
    "    overall_surface_pred.append(pred[0])\n",
    "    overall_volume_gt.append(gt[1])\n",
    "    overall_volume_pred.append(pred[1])\n",
    "    overall_population_gt.append(gt[2])\n",
    "    overall_population_pred.append(pred[2])\n",
    "    \n",
    "    # Create or update the city entry\n",
    "    if city not in cities_data:\n",
    "        cities_data[city] = {\n",
    "            \"N\": 0,\n",
    "            \"surface_gt\": [],\n",
    "            \"surface_pred\": [],\n",
    "            \"volume_gt\": [],\n",
    "            \"volume_pred\": [],\n",
    "            \"population_gt\": [],\n",
    "            \"population_pred\": []\n",
    "        }\n",
    "    cities_data[city][\"N\"] += 1\n",
    "    cities_data[city][\"surface_gt\"].append(gt[0])\n",
    "    cities_data[city][\"surface_pred\"].append(pred[0])\n",
    "    cities_data[city][\"volume_gt\"].append(gt[1])\n",
    "    cities_data[city][\"volume_pred\"].append(pred[1])\n",
    "    cities_data[city][\"population_gt\"].append(gt[2])\n",
    "    cities_data[city][\"population_pred\"].append(pred[2])\n",
    "\n",
    "# Define a function to compute metrics given true and predicted values\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return r2, mse, mae, rmse\n",
    "\n",
    "# Helper function to compute metrics if data is available\n",
    "def compute_metrics_for_target(gt_list, pred_list):\n",
    "    if len(gt_list) == 0:\n",
    "        return None\n",
    "    y_true = np.array(gt_list)\n",
    "    y_pred = np.array(pred_list)\n",
    "    return compute_metrics(y_true, y_pred)\n",
    "\n",
    "# Prepare a list to collect all rows for the final DataFrame.\n",
    "# We will have one row per target per city (plus an overall \"All\" row).\n",
    "rows = []\n",
    "\n",
    "# Define the targets as tuples: (display_name, key used in our lists)\n",
    "targets = [\n",
    "    (\"Total built-up surface area\", \"surface\"),\n",
    "    (\"Total built-up volume\", \"volume\"),\n",
    "    (\"Population\", \"population\")\n",
    "]\n",
    "\n",
    "# First, compute overall metrics (\"All\") using the overall lists.\n",
    "overall_data = {\n",
    "    \"surface\": (overall_surface_gt, overall_surface_pred),\n",
    "    \"volume\": (overall_volume_gt, overall_volume_pred),\n",
    "    \"population\": (overall_population_gt, overall_population_pred)\n",
    "}\n",
    "\n",
    "for target_name, key in targets:\n",
    "    gt_list, pred_list = overall_data[key]\n",
    "    metrics = compute_metrics_for_target(gt_list, pred_list)\n",
    "    if metrics is not None:\n",
    "        rows.append({\n",
    "            \"Target\": target_name,\n",
    "            \"City\": \"All\",\n",
    "            \"N\": len(gt_list),\n",
    "            \"R2\": metrics[0],\n",
    "            \"RMSE\": metrics[3],\n",
    "            \"MAE\": metrics[2]\n",
    "        })\n",
    "\n",
    "# Now, compute metrics for each city\n",
    "for city, data in cities_data.items():\n",
    "    for target_name, key in targets:\n",
    "        gt_list = data[f\"{key}_gt\"]\n",
    "        pred_list = data[f\"{key}_pred\"]\n",
    "        metrics = compute_metrics_for_target(gt_list, pred_list)\n",
    "        if metrics is not None:\n",
    "            rows.append({\n",
    "                \"Target\": target_name,\n",
    "                \"City\": city,\n",
    "                \"N\": data[\"N\"],\n",
    "                \"R2\": metrics[0],\n",
    "                \"RMSE\": metrics[3],\n",
    "                \"MAE\": metrics[2]\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the collected rows.\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# (Optional) Sort the DataFrame by Target and City for clarity.\n",
    "df = df.sort_values(by=[\"Target\", \"City\"]).reset_index(drop=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = \"/Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics_by_city.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Evaluation metrics saved to {output_csv_path}\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0           1            2           3            4   \\\n",
      "Target   Population  Population   Population  Population   Population   \n",
      "City            All     chicago     hongkong      kigali     kinshasa   \n",
      "N               897          87           78          84          102   \n",
      "R2         0.458813    -0.07485     0.377684    0.571203     0.586586   \n",
      "RMSE    2246.810225    661.5278  5399.925436  624.471638  2638.008609   \n",
      "\n",
      "                5           6           7            8            9   ...  \\\n",
      "Target  Population  Population  Population   Population   Population  ...   \n",
      "City        mexico      munich     orlando     saopaulo    singapore  ...   \n",
      "N               90          63         129           93           87  ...   \n",
      "R2        0.414827    0.391949    0.068959    -0.116943    -2.123055  ...   \n",
      "RMSE    857.510764  631.853814  220.264716  1192.303402  3656.419918  ...   \n",
      "\n",
      "                           23                     24                     25  \\\n",
      "Target  Total built-up volume  Total built-up volume  Total built-up volume   \n",
      "City                  chicago               hongkong                 kigali   \n",
      "N                          87                     78                     84   \n",
      "R2                   0.092899               0.745616               0.866056   \n",
      "RMSE               421.936978             406.680207              91.211664   \n",
      "\n",
      "                           26                     27                     28  \\\n",
      "Target  Total built-up volume  Total built-up volume  Total built-up volume   \n",
      "City                 kinshasa                 mexico                 munich   \n",
      "N                         102                     90                     63   \n",
      "R2                   0.797647               0.177581               0.103244   \n",
      "RMSE               116.728667             200.598459             168.442779   \n",
      "\n",
      "                           29                     30                     31  \\\n",
      "Target  Total built-up volume  Total built-up volume  Total built-up volume   \n",
      "City                  orlando               saopaulo              singapore   \n",
      "N                         129                     93                     87   \n",
      "R2                   0.436171              -1.131791              -0.366657   \n",
      "RMSE                122.15259             484.569541             528.480375   \n",
      "\n",
      "                           32  \n",
      "Target  Total built-up volume  \n",
      "City                stockholm  \n",
      "N                          84  \n",
      "R2                   0.764651  \n",
      "RMSE                 164.1831  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(\"/Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics_by_city.csv\")\n",
    "\n",
    "# Transpose the DataFrame\n",
    "df_transposed = df.T\n",
    "\n",
    "# Save the transposed DataFrame to a new CSV\n",
    "df_transposed.to_csv(\"/Users/wangzhuoyulucas/SMART /generatedImg/evaluation_metrics_by_city_transposed.csv\")\n",
    "\n",
    "print(df_transposed.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnet_gud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
